# -*- coding: utf-8 -*-
"""integration_clean_code with multi speaker trails.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16I22mHP_40eV9-Z5r5uNXRxBX_F5NfTB
"""

!git clone https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer.git

cd bark-voice-cloning-HuBERT-quantizer

!pip install -r requirements.txt

!pip install bark

pip install langdetect

pip install pyttsx3

pip install SpeechRecognition

pip install gtts

!pip install googletrans==4.0.0-rc1

!pip install pydub

!pip install librosa

import os
from bark.generation import load_codec_model
from encodec.utils import convert_audio
from bark_hubert_quantizer.hubert_manager import HuBERTManager
from bark_hubert_quantizer.pre_kmeans_hubert import CustomHubert
from bark_hubert_quantizer.customtokenizer import CustomTokenizer
import torchaudio
import torch
import numpy as np
from bark.api import generate_audio
from IPython.display import Audio
from bark.generation import SAMPLE_RATE
from google.colab import files
from googletrans import Translator, constants
import speech_recognition as sr
from langdetect import detect

print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("Number of GPUs:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("GPU name:", torch.cuda.get_device_name(0))

device = 'cuda' if torch.cuda.is_available() else 'cpu'
# model = load_codec_model(use_gpu=True if device == 'cuda' else False)

model = load_codec_model(use_gpu=True if device == 'cuda' else False)

# Function to recognize speech from audio data
def recognize_audio(audio_data, language=None):
    r = sr.Recognizer()
    try:
        if language:
            text = r.recognize_google(audio_data, language=language)
        else:
            detected_language = detect(r.recognize_google(audio_data))
            text = r.recognize_google(audio_data, language=detected_language)
            print("Detected language:", detected_language)
        return text, language if language else detected_language
    except sr.UnknownValueError:
        print("Google Speech Recognition could not understand audio")
        return "", ""
    except sr.RequestError as e:
        print("Could not request results from Google Speech Recognition service; {0}".format(e))
        return "", ""

# Function to translate text to a specified language
def translate_text(text, dest_lang='en'):
    translator = Translator()
    translated = translator.translate(text, dest=constants.LANGUAGES[dest_lang])
    return translated.text

# Function to get the target language for translation
def get_target_language():
    print("Enter the language code for translation (e.g., fr for French):")
    language_code = input("Language Code: ").strip()
    return language_code

# Main function for integrated process
def main():
    # Create directory for voice cloning prompt if it doesn't exist
    if not os.path.exists('bark'):
        os.makedirs('bark')

    # Audio file upload and processing
    print("Upload the recorded audio file:")
    audio_file = files.upload()
    filename = list(audio_file.keys())[0]

    # Ask for the input language code
    input_language = input("Enter the input language code (optional): ").strip()

    # Load HuBERT model for voice cloning
    hubert_manager = HuBERTManager()
    hubert_manager.make_sure_hubert_installed()
    hubert_manager.make_sure_tokenizer_installed()
    hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt').to(device)
    tokenizer = CustomTokenizer.load_from_checkpoint('data/models/hubert/tokenizer.pth').to(device)

    # Load and process audio file for voice cloning
    wav, sar = torchaudio.load(filename)
    wav = convert_audio(wav, sar, model.sample_rate, model.channels)
    wav = wav.to(device)
    semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)
    semantic_tokens = tokenizer.get_token(semantic_vectors)

    # Encode frames using HuBERT model
    with torch.no_grad():
        encoded_frames = model.encode(wav.unsqueeze(0))
    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]
    codes = codes.cpu().numpy()
    semantic_tokens = semantic_tokens.cpu().numpy()

    # Save encoded prompts for voice cloning
    output_path = 'bark/voice_cloning_prompt.npz'
    np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)

    # Recognize speech from uploaded audio
    audio_data = sr.AudioFile(filename)
    with audio_data as source:
        audio = sr.Recognizer().record(source)
    recognized_text, detected_language = recognize_audio(audio, language=input_language if input_language else None)

    # Ask for the output translation language
    target_language = get_target_language()

    # Translate the recognized text
    translated_text = translate_text(recognized_text, dest_lang=target_language)
    print("Translated text:", translated_text)

    # Generate speech in cloned voice from translated text
    audio_array = generate_audio(translated_text, history_prompt=output_path, text_temp=0.7, waveform_temp=0.7)

    # Play the synthesized audio in the cloned voice
    display(Audio(audio_array, rate=SAMPLE_RATE))

if __name__ == "__main__":
    main()

pip install pvfalcon
!pip install pydub

# Commented out IPython magic to ensure Python compatibility.
import os
import pvfalcon
from pydub import AudioSegment
import torchaudio
import torch
import numpy as np
from bark.generation import load_codec_model, generate_audio, SAMPLE_RATE
from encodec.utils import convert_audio
from bark_hubert_quantizer.hubert_manager import HuBERTManager
from bark_hubert_quantizer.pre_kmeans_hubert import CustomHubert
from bark_hubert_quantizer.customtokenizer import CustomTokenizer
from googletrans import Translator, constants
import speech_recognition as sr
from langdetect import detect
from IPython.display import Audio

# Initialize the pvfalcon library
falcon = pvfalcon.create(access_key="ESj9DA04ueBKgpOflb2VGAQ4B0J+c9haXjJ4RTyg2F3hxKNZFrTJ6Q==")

# Process the audio file to get segments
segments = falcon.process_file("/content/kovoiceinhindi_NgAIVpoO.wav")

# Print segment information
for segment in segments:
    print(
        "{speaker_tag=%d start_sec=%.2f end_sec=%.2f}"
#         % (segment.speaker_tag, segment.start_sec, segment.end_sec)
    )

# Load the input audio file
input_audio = AudioSegment.from_wav("/content/kovoiceinhindi_NgAIVpoO.wav")

# Create a directory to save the segmented audio files
output_dir = "/content/segmented_audio"
os.makedirs(output_dir, exist_ok=True)

# Process each segment
for i, segment in enumerate(segments):
    start_ms = int(segment.start_sec * 1000)  # Convert start time to milliseconds
    end_ms = int(segment.end_sec * 1000)  # Convert end time to milliseconds
    speaker_tag = segment.speaker_tag

    # Extract the segment from the input audio
    segment_audio = input_audio[start_ms:end_ms]

    # Save the segment to a separate audio file
    segment_path = os.path.join(output_dir, f"segment_{i}_speaker_{speaker_tag}.wav")
    segment_audio.export(segment_path, format="wav")

    print(f"Segment {i}: Speaker {speaker_tag}, Start: {segment.start_sec:.2f}s, End: {segment.end_sec:.2f}s saved to {segment_path}")

# Part 2: Translation and Voice Synthesis

print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("Number of GPUs:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("GPU name:", torch.cuda.get_device_name(0))

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = load_codec_model(use_gpu=True if device == 'cuda' else False)

# Function to recognize speech from audio data
def recognize_audio(audio_data, language=None):
    r = sr.Recognizer()
    try:
        if language:
            text = r.recognize_google(audio_data, language=language)
        else:
            detected_language = detect(r.recognize_google(audio_data))
            text = r.recognize_google(audio_data, language=detected_language)
            print("Detected language:", detected_language)
        return text, language if language else detected_language
    except sr.UnknownValueError:
        print("Google Speech Recognition could not understand audio")
        return "", ""
    except sr.RequestError as e:
        print("Could not request results from Google Speech Recognition service; {0}".format(e))
        return "", ""

# Function to translate text to a specified language
def translate_text(text, dest_lang='en'):
    translator = Translator()
    translated = translator.translate(text, dest=dest_lang)
    return translated.text

# Function to get the target language for translation
def get_target_language():
    print("Enter the language code for translation (e.g., fr for French):")
    language_code = input("Language Code: ").strip()
    return language_code

# Load HuBERT model for voice cloning
hubert_manager = HuBERTManager()
hubert_manager.make_sure_hubert_installed()
hubert_manager.make_sure_tokenizer_installed()
hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt').to(device)
tokenizer = CustomTokenizer.load_from_checkpoint('data/models/hubert/tokenizer.pth').to(device)

# Ask for the output translation language
target_language = get_target_language()

# Process each segmented audio file
for i, segment in enumerate(segments):
    segment_path = os.path.join(output_dir, f"segment_{i}_speaker_{segment.speaker_tag}.wav")

    # Load and process audio file for voice cloning
    wav, sar = torchaudio.load(segment_path)
    wav = convert_audio(wav, sar, model.sample_rate, model.channels)
    wav = wav.to(device)
    semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)
    semantic_tokens = tokenizer.get_token(semantic_vectors)

    # Encode frames using HuBERT model
    with torch.no_grad():
        encoded_frames = model.encode(wav.unsqueeze(0))
    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]
    codes = codes.cpu().numpy()
    semantic_tokens = semantic_tokens.cpu().numpy()

    # Save encoded prompts for voice cloning
    output_prompt_path = os.path.join(output_dir, f"voice_cloning_prompt_{i}_speaker_{segment.speaker_tag}.npz")
    np.savez(output_prompt_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)

    # Recognize speech from the segmented audio
    audio_data = sr.AudioFile(segment_path)
    with audio_data as source:
        audio = sr.Recognizer().record(source)
    recognized_text, detected_language = recognize_audio(audio)

    # Translate the recognized text
    translated_text = translate_text(recognized_text, dest_lang=target_language)
    print(f"Segment {i}: Translated text:", translated_text)

    # Generate speech in cloned voice from translated text
    audio_array = generate_audio(translated_text, history_prompt=output_prompt_path, text_temp=0.7, waveform_temp=0.7)

    # Save and play the synthesized audio in the cloned voice
    output_audio_path = os.path.join(output_dir, f"translated_segment_{i}_speaker_{segment.speaker_tag}.wav")
    torchaudio.save(output_audio_path, torch.tensor(audio_array).unsqueeze(0), SAMPLE_RATE)
    display(Audio(audio_array, rate=SAMPLE_RATE))